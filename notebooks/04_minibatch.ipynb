{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707bcdbe-22f0-49b3-8dc7-c659c2ea3431",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Minibatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fbab1c-8e99-4d54-944a-48ec19323069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbdbc34-ab5e-46a6-95f9-0a12cae1e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# Get the MNIST dataset\n",
    "import gzip, pickle\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "MNIST_URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true\"\n",
    "data_path = Path(\"../data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "data_gz = data_path/\"mnist.pkl.gz\"\n",
    "\n",
    "# Get the data\n",
    "if not data_gz:\n",
    "    urllib.request.urlretrieve(MNIST_URL, data_path/\"mnist.pkl.gz\")\n",
    "    \n",
    "# Destructuring\n",
    "with gzip.open(data_gz, mode='rb') as unzip_data:\n",
    "    obj = pickle.load(unzip_data, encoding=\"latin-1\")\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = obj\n",
    "\n",
    "# To tensors\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e20edea-32a5-4222-b178-084a0ffe33da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = int(y_train.max()+1)\n",
    "nh = 50\n",
    "n, m, c, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d3b312-bf06-4f05-85b9-b4bdd936c37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our model architecture from 03_backpropagation.ipynb\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(n_in, nh), # [784,50]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nh, n_out) # [50,10]\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27adc51d-9e93-4b95-9363-34c03f952832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(x_train)\n",
    "preds.shape # We are now going to use 10 categories for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c34768-f492-43cb-a0af-79f0046f223c",
   "metadata": {},
   "source": [
    "## Implement `Cross Entropy Loss` function\n",
    "\n",
    "Compared to the last notebook, here, we are going to predict 10 categogies for the output. \n",
    "\n",
    "So, the outputs -- from the model -- will be probabilities of the predictions for the MNIST numbers.\n",
    "\n",
    "The targets will be `on-hot-encoded` vectors with the index of a 1 representing the actual number.\n",
    "\n",
    "That means that we need to change our `loss` function from `MSE` to `Cross Entropy Loss`.\n",
    "\n",
    "`Cross Entropy Loss` typically serves multi-class and multi-label classifications.\n",
    "\n",
    "`Cross Entropy Loss = log(SoftMax(i))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708c08b-4294-4008-9568-fd6fd6a9b39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30ef424-4573-44a5-9da9-7817257cc995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (x.exp()/(x.exp().sum(dim=-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07ebf3-40ac-4543-be54-abcccd956b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62f1635-6451-436d-ab2b-ef801fc87afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#log(softmax(x)):\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dac5435-ced7-4bf6-a6b3-465a67a01538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1322, -2.3039, -2.1242,  ..., -2.2312, -2.5709, -2.4597],\n",
       "        [-2.1817, -2.3757, -2.2375,  ..., -2.1507, -2.6064, -2.3893],\n",
       "        [-2.2826, -2.3179, -2.3498,  ..., -2.3222, -2.3067, -2.4729],\n",
       "        ...,\n",
       "        [-2.2142, -2.2712, -2.2377,  ..., -2.2306, -2.4570, -2.3492],\n",
       "        [-2.1171, -2.1884, -2.3784,  ..., -2.3294, -2.3905, -2.3748],\n",
       "        [-2.2644, -2.2546, -2.2944,  ..., -2.2859, -2.3665, -2.3277]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds = log_softmax(preds)\n",
    "log_softmax_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83731144-ecc3-47ee-8a0b-7f6afede400b",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "In `PyTorch` this is known as : negative log likelihood loss == `nll`\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807cfd21-17a2-4e8b-9b74-0187ae04b938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 3 numbers\n",
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2013c7-e899-4279-9103-fce7ab8c98d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c2b741-ab3d-470f-85f7-19a8d2e8ae10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2618, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.1817, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.3327, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prababilities of the categories from above are\n",
    "log_softmax_preds[0, 5], log_softmax_preds[1, 0], log_softmax_preds[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f900fddc-41ab-4ac7-bc07-766f95bb363d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2618, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.1817, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.3327, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[0, y_train[0]], log_softmax_preds[1, y_train[1]], log_softmax_preds[2, y_train[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4751548-7589-41bb-b5a4-7a2df9d7e68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2618, -2.1817, -2.3817], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[[0,1,3], y_train[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c479ba9-c851-43ab-accb-60c0fc78598a",
   "metadata": {},
   "source": [
    "So, we have the `log(softmax(p))`. Now we need to implement the Cross entropy loss from above as the sum of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f94afa-f14e-4203-a7b4-bce7c7ee01d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nll(inp, target):\n",
    "    \"\"\" Cross Entropy Loss \"\"\"\n",
    "    return -inp[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49fe07f-65be-49d9-a62f-86501697f504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3204, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(log_softmax_preds, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57448954-1669-4a23-b31b-30c63c68ad16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Going to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c906e-4b44-4bd4-a726-89ea6e75746a",
   "metadata": {},
   "source": [
    "`PyTorch` already has a function for `Cross Entropy Loss`, so we can use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef8efe8-3830-4b0e-b7d1-d5e9df840c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3204, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# nll = negative log likelihood loss\n",
    "F.nll_loss(F.log_softmax(preds, dim=1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e861-e21a-4ea0-9557-bd0adf4de8b3",
   "metadata": {},
   "source": [
    "The above combination of the 2 functions can be combined into 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487d960c-da9e-4ad2-9a25-50ef2e878cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3204, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f6b92-af96-4603-921e-741c3a5b01c0",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "1) get the predictions from the model\n",
    "2) calculate the loss from the predictions (based on y_train)\n",
    "3) calculate the gradients of the loss with respect to every parameter of the model\n",
    "4) adjust the parameters according to their gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca9d642-5bc9-40c6-9aa3-23e136b27f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa881109-ee28-487c-9f56-0cefacbc5c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1658, -0.0059,  0.1738, -0.0087, -0.2291,  0.0362,  0.0821,  0.0668,\n",
       "         -0.2729, -0.1617], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[:bs] # minibatch from x\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f046e1c4-ac61-47f1-8917-6bf0a52492f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:64]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79f255b-a44c-4926-b807-103aed69a9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3166, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3635a4-7bb0-4149-a001-193b89a1b406",
   "metadata": {
    "tags": []
   },
   "source": [
    "We need to grab the highest probabilities of the predicted nubmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f921d26-059d-4d99-b3b0-2f8e2e861e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 5, 5, 0, 2, 0, 5, 0, 0, 0, 0, 3, 5, 0, 2, 5, 0, 5, 0, 5, 0, 2, 5,\n",
       "        7, 0, 0, 6, 7, 5, 3, 2, 2, 6, 0, 0, 2, 7, 0, 5, 0, 0, 0, 6, 0, 0, 0, 0,\n",
       "        0, 0, 0, 3, 6, 5, 6, 0, 7, 2, 3, 5, 6, 5, 5, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362455d-3fa0-419b-9e97-3966800aa603",
   "metadata": {},
   "source": [
    "And we define the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e4506e-24d5-4f31-b26b-a5c5b70e97b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy(preds, targs):\n",
    "    \"\"\" The average of the correctly predicted numbers \"\"\"\n",
    "    return (preds.argmax(dim=1)==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8268c9f1-b240-4121-b0a1-dde89b17affb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0781)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c244910-25dc-4e1d-8fed-6c4c10fff62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(epoch, preds, targs, loss):\n",
    "    \"\"\" Print a report after each epoch of training \"\"\"\n",
    "    print(f\"epoch:{epoch} \\t accuracy:{accuracy(preds, targs).item():.3f} \\t loss:{loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3f0e-7932-468b-b4c0-06fbc4404b35",
   "metadata": {},
   "source": [
    "We have an accuracy of ~10% which is expected because of the random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4d8c3-1587-479d-a2fd-7889d6140d86",
   "metadata": {},
   "source": [
    "Let's setup the basic training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2203c8da-124f-4efd-9f55-378740cdadcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recreate the model before running the training\n",
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d4cb29-f186-4617-99f4-14a2ae088046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.078125 \t loss:2.2974798679351807\n",
      "epoch:1 \t accuracy:0.96875 \t loss:0.0978163555264473\n",
      "epoch:2 \t accuracy:0.984375 \t loss:0.06935223191976547\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        \n",
    "        # Create xb, yb batch - this operation is way slower than slice\n",
    "        if i+i+batch_size > x_train.shape[0]:\n",
    "            xb = x_train[i:]\n",
    "            yb = y_train[i:]\n",
    "        else:\n",
    "            xb = x_train[i:i+batch_size] # x batch\n",
    "            yb = y_train[i:i+batch_size] # y batch\n",
    "        \n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        if i == 0:\n",
    "            # End of each epoch\n",
    "            print(f\"epoch:{epoch} \\t accuracy:{accuracy(preds, yb).item()} \\t loss:{loss.item()}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830b391-71ed-4a58-b650-865aebebc756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using `slice`\n",
    "\n",
    "The `slice()` function returns a `slice object` that is used to slice any sequence (string, tuple, list, range, or bytes). So, we can use it to slice the training tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2712c226-61c2-4962-9245-375d797f5efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9186d6de-f896-4283-80b5-eac793197016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.31061\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.07262\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.04642\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "n_inp = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n_inp, 64):\n",
    "        s = slice(i, min(n_inp, i + bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "719acf09-274e-49d9-88d2-e2262c5df87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = slice(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50816553-44b8-466c-b924-ad8a352a2650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[s].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb09dd-e6df-4a1a-b435-b2171f80de69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
