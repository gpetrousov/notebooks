{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707bcdbe-22f0-49b3-8dc7-c659c2ea3431",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Minibatch\n",
    "\n",
    "\n",
    "Why are we re-writing things from scratch?\n",
    "\n",
    "The idea is that we're trying to learn a library better. If we can't learn what the library does,  we can just re-implement the algorithm ourselves and then we'll be able to understand the library's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fbab1c-8e99-4d54-944a-48ec19323069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4bbdbc34-ab5e-46a6-95f9-0a12cae1e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Get the MNIST dataset\n",
    "import gzip, pickle\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MNIST_URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true\"\n",
    "data_path = Path(\"../data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "data_gz = data_path/\"mnist.pkl.gz\"\n",
    "\n",
    "# Get the data\n",
    "if not data_gz:\n",
    "    urllib.request.urlretrieve(MNIST_URL, data_path/\"mnist.pkl.gz\")\n",
    "    \n",
    "# Destructuring\n",
    "with gzip.open(data_gz, mode='rb') as unzip_data:\n",
    "    obj = pickle.load(unzip_data, encoding=\"latin-1\")\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = obj\n",
    "\n",
    "# To tensors\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e20edea-32a5-4222-b178-084a0ffe33da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = int(y_train.max()+1)\n",
    "nh = 50\n",
    "n, m, c, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d3b312-bf06-4f05-85b9-b4bdd936c37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our model architecture from 03_backpropagation.ipynb\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(n_in, nh), # [784,50]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nh, n_out) # [50,10]\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27adc51d-9e93-4b95-9363-34c03f952832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(x_train)\n",
    "preds.shape # We are now going to use 10 categories for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c34768-f492-43cb-a0af-79f0046f223c",
   "metadata": {},
   "source": [
    "## Implement `Cross Entropy Loss` function\n",
    "\n",
    "In the last notebook, we used the MSE (L2-norm) to calculate the error. That function was not ideal for predicting multicategory classifications, like MNIST. Following, we are going to reconstruct the `cross entropy loss` function from scratch. \n",
    "\n",
    "The targets will be `on-hot-encoded` vectors with the index of a 1 representing the actual number.\n",
    "\n",
    "**`Cross Entropy Loss` typically serves multi-class and multi-label classifications.**\n",
    "\n",
    "`Cross Entropy Loss = log(SoftMax(i))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708c08b-4294-4008-9568-fd6fd6a9b39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30ef424-4573-44a5-9da9-7817257cc995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# softmax: x.exp()/(x.exp().sum(dim=-1, keepdim=True))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax \"\"\"\n",
    "    return (x.exp()/(x.exp().sum(dim=-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07ebf3-40ac-4543-be54-abcccd956b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62f1635-6451-436d-ab2b-ef801fc87afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#log(softmax(x)):\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dac5435-ced7-4bf6-a6b3-465a67a01538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2807, -2.5093, -2.3227,  ..., -2.2554, -2.3243, -2.1333],\n",
       "        [-2.4135, -2.4360, -2.2427,  ..., -2.3645, -2.1714, -2.0787],\n",
       "        [-2.3150, -2.3989, -2.1929,  ..., -2.3479, -2.1501, -2.1698],\n",
       "        ...,\n",
       "        [-2.3431, -2.3722, -2.2730,  ..., -2.2801, -2.2319, -2.1322],\n",
       "        [-2.3011, -2.4860, -2.3119,  ..., -2.2380, -2.1589, -2.2805],\n",
       "        [-2.3057, -2.4393, -2.1590,  ..., -2.3203, -2.2708, -2.2438]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds = log_softmax(preds)\n",
    "log_softmax_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83731144-ecc3-47ee-8a0b-7f6afede400b",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "In `PyTorch` this is known as : negative log likelihood loss == `nll`\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807cfd21-17a2-4e8b-9b74-0187ae04b938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 3 numbers\n",
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2013c7-e899-4279-9103-fce7ab8c98d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c2b741-ab3d-470f-85f7-19a8d2e8ae10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2067, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.4135, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.4659, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prababilities of the categories from above are\n",
    "log_softmax_preds[0, 5], log_softmax_preds[1, 0], log_softmax_preds[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f900fddc-41ab-4ac7-bc07-766f95bb363d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2067, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.4135, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.4659, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[0, y_train[0]], log_softmax_preds[1, y_train[1]], log_softmax_preds[2, y_train[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4751548-7589-41bb-b5a4-7a2df9d7e68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2067, -2.4135, -2.4659], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[[0,1,2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c479ba9-c851-43ab-accb-60c0fc78598a",
   "metadata": {},
   "source": [
    "So, we have the `log(softmax(p))`. Now we need to implement the Cross entropy loss from above as the sum of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f94afa-f14e-4203-a7b4-bce7c7ee01d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nll(inp, target):\n",
    "    \"\"\" Cross Entropy Loss \"\"\"\n",
    "    return -inp[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49fe07f-65be-49d9-a62f-86501697f504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3065, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(log_softmax_preds, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57448954-1669-4a23-b31b-30c63c68ad16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Going to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c906e-4b44-4bd4-a726-89ea6e75746a",
   "metadata": {},
   "source": [
    "`PyTorch` already has a function for `Cross Entropy Loss`, so we can use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef8efe8-3830-4b0e-b7d1-d5e9df840c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3065, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# nll = negative log likelihood loss\n",
    "F.nll_loss(F.log_softmax(preds, dim=1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e861-e21a-4ea0-9557-bd0adf4de8b3",
   "metadata": {},
   "source": [
    "The above combination of the 2 functions can be combined into 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487d960c-da9e-4ad2-9a25-50ef2e878cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3065, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f6b92-af96-4603-921e-741c3a5b01c0",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "1) get the predictions from the model\n",
    "2) calculate the loss from the predictions (based on y_train)\n",
    "3) calculate the gradients of the loss with respect to every parameter of the model\n",
    "4) adjust the parameters according to their gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca9d642-5bc9-40c6-9aa3-23e136b27f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa881109-ee28-487c-9f56-0cefacbc5c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0984, -0.1302,  0.0564, -0.0200,  0.0017,  0.1724,  0.1127,  0.1237,\n",
       "          0.0548,  0.2458], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[:bs] # minibatch from x\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f046e1c4-ac61-47f1-8917-6bf0a52492f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:64]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79f255b-a44c-4926-b807-103aed69a9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3205, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3635a4-7bb0-4149-a001-193b89a1b406",
   "metadata": {
    "tags": []
   },
   "source": [
    "We need to grab the highest probabilities of the predicted nubmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f921d26-059d-4d99-b3b0-2f8e2e861e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 8, 7, 5, 8, 8, 5, 8, 2, 9, 8, 9, 8, 8, 5, 9, 2, 8, 2, 5, 9, 2, 7,\n",
       "        9, 9, 8, 2, 7, 2, 2, 2, 5, 8, 5, 2, 5, 8, 2, 7, 2, 2, 9, 2, 9, 5, 8, 5,\n",
       "        9, 9, 7, 5, 2, 8, 5, 8, 5, 5, 5, 8, 2, 8, 9, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362455d-3fa0-419b-9e97-3966800aa603",
   "metadata": {},
   "source": [
    "And we define the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34e4506e-24d5-4f31-b26b-a5c5b70e97b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy(preds:torch.tensor, targs:torch.tensor) -> float:\n",
    "    \"\"\" The average of the correctly predicted numbers \"\"\"\n",
    "    return (preds.argmax(dim=1)==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8268c9f1-b240-4121-b0a1-dde89b17affb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4c244910-25dc-4e1d-8fed-6c4c10fff62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(epoch:int, preds:torch.tensor, targs:torch.tensor, loss:float):\n",
    "    \"\"\" Print a report after each epoch of training \"\"\"\n",
    "    print(f\"epoch:{epoch} \\t accuracy:{accuracy(preds, targs).item():.3f} \\t loss:{loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de3f0e-7932-468b-b4c0-06fbc4404b35",
   "metadata": {},
   "source": [
    "We have an accuracy of ~10% which is expected because of the random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4d8c3-1587-479d-a2fd-7889d6140d86",
   "metadata": {},
   "source": [
    "Let's setup the basic training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2203c8da-124f-4efd-9f55-378740cdadcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recreate the model before running the training\n",
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d4cb29-f186-4617-99f4-14a2ae088046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.875000 \t loss:0.289887\n",
      "epoch:1 \t accuracy:1.000000 \t loss:0.109148\n",
      "epoch:2 \t accuracy:1.000000 \t loss:0.031370\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        \n",
    "        # Create xb, yb batch - this operation is way slower than slice\n",
    "        if i+batch_size > x_train.shape[0]:\n",
    "            xb = x_train[i:]\n",
    "            yb = y_train[i:]\n",
    "        else:\n",
    "            xb = x_train[i:i+batch_size] # x batch\n",
    "            yb = y_train[i:i+batch_size] # y batch\n",
    "            \n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    print(f\"epoch:{epoch} \\t accuracy:{accuracy(preds, yb).item():.6f} \\t loss:{loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830b391-71ed-4a58-b650-865aebebc756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using `slice`\n",
    "\n",
    "The `slice()` function returns a `slice object` that is used to slice any sequence (string, tuple, list, range, or bytes). So, we can use it to slice the training tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2712c226-61c2-4962-9245-375d797f5efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9186d6de-f896-4283-80b5-eac793197016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.28130\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.05041\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.03098\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "n_inp = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n_inp, 64):\n",
    "        s = slice(i, min(n_inp, i + bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4062cd-8fa0-45e1-a9bf-73869919bbb7",
   "metadata": {},
   "source": [
    "### PyTorch parameters\n",
    "\n",
    "We are going to reduce the code size for the parameter update.\n",
    "\n",
    "If we create an object which inherits from `nn.Module`, we can assign arbitrary parameters to it and traverse them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eba11b9c-f657-4d38-b119-15c961aec37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (bar): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = nn.Module()\n",
    "m1.foo = nn.Linear(3, 4)\n",
    "m1.bar = nn.Linear(4, 1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9a08786-500e-489a-a17f-8e8e60330204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True)),\n",
       " ('bar', Linear(in_features=4, out_features=1, bias=True))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows us to access the attributes of m1\n",
    "list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0299eae-8091-41a6-b784-e3c6b3dff9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1181, -0.0741, -0.3407],\n",
       "         [-0.1159,  0.3236, -0.3029],\n",
       "         [ 0.4393,  0.0018, -0.0367],\n",
       "         [-0.0434, -0.0732,  0.1620]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5265,  0.3642, -0.4719,  0.4090], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3864,  0.4349,  0.1469, -0.3742]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.4837], requires_grad=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prints the weights and bias matrices\n",
    "list(m1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4502bac4-ad18-4e8f-a7d1-52d5d9bbf7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Simple Multi Layer Perceptron to illustrate the use of parameter. \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cf5f17d-bf18-4c7e-a92b-881e7d5cd204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(m, nh, c)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f55128d-2df0-459b-bb87-b639859d58c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children():\n",
    "    print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf07a0f-e000-41f6-966c-a576e8b0864f",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can access the parameteres (`weights` and `biases`) of each layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecf5bb8f-90f7-4e1c-bdbc-bc1fcaf694bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e18ef-c1cc-485e-9f31-620ea430db57",
   "metadata": {},
   "source": [
    "That means, we can update them in the loop we hade defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f7ba91b-8c7e-41ba-98e5-cf0c42f353f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLP(m, nh, c)\n",
    "bs = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "n_inp = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3bc366d7-a2d4-449e-a2c7-fe53cc530519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model=model, epochs=3):\n",
    "    \"\"\" Basic loop using Torch model parameters to update their gradients. \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n_inp, bs):\n",
    "            s = slice(i, min(n_inp, i+bs))\n",
    "            xb = x_train[s]\n",
    "            yb = y_train[s]\n",
    "            preds = model(xb) # We are now using the MLP model\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad*lr\n",
    "                model.zero_grad() # Reset gradients for all model parameters\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9211cdef-9694-4e34-83e0-6fef47ea52b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.20877\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.09138\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.06136\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49880cec-c432-4962-a632-68658265a0da",
   "metadata": {},
   "source": [
    "#### How it works in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e610d333-6216-4e03-9473-2f0643b8b69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self, k, v):\n",
    "        \"\"\" This is how PyTorch.nn.Module stores the children of the model. \"\"\"\n",
    "        if not k.startswith(\"_\"):\n",
    "            self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._modules}\"\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\" How to traverse parametes \"\"\"\n",
    "        for l in self._modules.values():\n",
    "            yield from l.parameters()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4bcf22c1-f957-48e3-926d-a1744c0b02eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel = MyModule(m, nh, c)\n",
    "mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4bb4b920-2837-4526-b79e-4f977752c985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mymodel.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532e8c4-82b9-4684-8067-3bf5ea8dddb1",
   "metadata": {},
   "source": [
    "### Registering layers in model - from scratch\n",
    "\n",
    "How do we registers all the layers in our model at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9637faa8-6bdb-49d2-97f8-f5806d40aa1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppose we store the layers in a variable\n",
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "46289d90-2417-4919-9684-f93c682baa24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(self.layers):\n",
    "            self.add_module(f\"layer_{i}\", l) # This adds the modules to the module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d36a685f-a516-4f75-af42-864ea15a8fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6906215-211a-4b1b-b932-c283771da10b",
   "metadata": {},
   "source": [
    "It's apparent that **we're building a sequential model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cbf73-6978-4fe5-b129-d31cf5f91847",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `nn.ModuleList()`\n",
    "\n",
    "`PyTorch` allows use to add module with the `nn.ModuleList()` function.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "75e9e879-2286-4b5d-8da3-348c55b14d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(layers) # Create sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = l(x)\n",
    "        return x\n",
    "        # return reduce(lambda val,layer: layer(val), self.layers, x)\n",
    "    # Python reduce: https://realpython.com/python-reduce-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc15addb-92de-4a72-9943-aff433a2929a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqmodel = SequentialModel(layers)\n",
    "seqmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "30f5c9fe-650d-4eb8-8b02-4072c28e083c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.26509\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.05742\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.03991\n"
     ]
    }
   ],
   "source": [
    "fit(model=seqmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bc782-3db1-471b-a71d-f1ad70375d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `nn.Sequential()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "097275c0-be0f-44d0-af77-245e1329d2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6e17ff4b-60e4-4c8d-b078-57e27ea6f827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.26006\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.12742\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.04654\n"
     ]
    }
   ],
   "source": [
    "fit(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfe185-ceaa-439f-8057-8bea42d3e878",
   "metadata": {},
   "source": [
    "### SGD Optimizer - from scratch\n",
    "\n",
    "Finding the gradients of the layers and updating the parameters is a common step. An optimizer is a function which does that for us.\n",
    "\n",
    "```Python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= p.grad*lr\n",
    "    model.zero_grad() # Reset gradients for all model parameters\n",
    "```\n",
    "\n",
    "Let's create our own `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d930256a-2fcc-4ff8-9edd-09baf19c2691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6ed54b23-9754-4437-9f78-6aa4a36005d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a016ea31-c812-44f2-b441-9822d2c57366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create opt as an object of the model's parameters\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1220c91a-aa78-4d14-8812-f7ed8a9afab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.27777\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.12077\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.06665\n"
     ]
    }
   ],
   "source": [
    "# New training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(i+bs, n))\n",
    "        xb = x_train[s]\n",
    "        yb = y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Out custom optimizer\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5795599-cea4-4fe0-9d8b-f96c5ddef263",
   "metadata": {
    "tags": []
   },
   "source": [
    "PyTorch provides the same functionality with `optim.SGD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf08ae2-f3b0-4273-85e6-d803bc6feca0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4e0d1133-b626-4b8e-b90a-6edb3089be1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f9adc040-1c7d-410b-b25c-1919c52be9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "opt = optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6b3ef14c-f08f-4c73-9975-a1ab0b284a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.24661\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.08220\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.06311\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(i+bs, n))\n",
    "        xb = x_train[s]\n",
    "        yb = y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # PyTorch's SGD optimizer\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3dbfb062-ba1f-438d-b183-1d978646564e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_lin_model():\n",
    "    _model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "    _opt = optim.SGD(_model.parameters(), lr=0.5)\n",
    "    return _model, _opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3842aa-dbce-4951-a25a-08c4d33e4be0",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "We're going to create the `Dataset` and `DataLoader` from scratch to understand how they're implemented in `PyTorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bb5bc-b130-4ffb-9dca-143fd81d5d00",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We'll go from:\n",
    "\n",
    "```python\n",
    "s = slice(i, min(i+bs, n))\n",
    "xb = x_train[s]\n",
    "yb = y_train[s]\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "xb,yb = train_ds[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "138e3750-e64b-4452-aac7-75cc029a15b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, s):\n",
    "        \"\"\" Slicing function: replaced slice() \"\"\"\n",
    "        return self.x[s], self.y[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a186c205-ed87-4e01-9f02-13f3bc0c55f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9e22da11-38f2-496f-ad66-024288c06871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7027e1b4-6f42-44ec-b199-5f495f07b083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "de2ccc36-758d-4768-91a6-0f95dc8d8294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New fit function with Dataset class\n",
    "def fit(model = model, opt = opt):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            xb, yb = train_ds[i:min(n, i+bs)]\n",
    "            preds = model(xb)\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ad01d82f-878d-421d-b2c5-85169423255a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.20341\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.08491\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.05776\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105db3c3-1f15-4e8d-90ad-cdb45b0ea2f1",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "We are going to replace:\n",
    "\n",
    "```Python\n",
    "for i in range(0, n, bs):\n",
    "    xb, yb = train_ds[i:min(n, i+bs)]\n",
    "```\n",
    "with:\n",
    "```Python\n",
    "for xb, yb in train_dl:\n",
    "    ...\n",
    "```\n",
    "\n",
    "So we need to create an itterator with `__iter__()` and `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7cbd67d1-3773-4353-b947-e64905bd287e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:min(len(self.ds), i+self.bs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "33330811-9123-47e3-8ec5-76ce4c2ee88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "978daa37-7670-47c7-ab75-cb5966618092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c4512aad-f598-40fa-b39c-7d30e96a83a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "05de6684-0e80-4911-9d05-5125c7a46289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe56ab4f250>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[7].view(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e62c773a-5205-4a8f-823f-b32770b50613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "48c244c7-dbd0-4d72-b9a7-c4ec7d4bdea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4cdb2b70-0b8e-48f7-9c2e-a7f2c6f51a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.875 \t loss:0.25264\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.16140\n",
      "epoch:2 \t accuracy:0.938 \t loss:0.10023\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b567e5-eae4-4681-9176-634167203a36",
   "metadata": {},
   "source": [
    "### Batch randomizer\n",
    "\n",
    "Now, we have created a way to load datasets into the traininig function. However, the DataLoaderes are sequential - they fetch items in a sequence.\n",
    "\n",
    "One feature we can add to the `DataLoader` is a shuffler, which fetches random data from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1728c9e5-e45f-4864-8d22-9ee4ab3019f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a69a98be-89b5-4f83-8a32-f76dd23961bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    \n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the indexes which we're going to use to create xb and yb\"\"\"\n",
    "        result = list(range(self.n))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(result)\n",
    "        return iter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "54852296-e71c-4510-9e6f-60e3b6e79047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "04b76b86-f2e9-4b7e-b6e7-b5fa91dfd9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds) # If shuffle==False\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "103daefa-4a07-4157-98cf-a71887a6393d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27911, 9797, 3179, 6012, 2541]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=True)\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a448c-ecce-4521-80ad-d9ed2e4c0dc0",
   "metadata": {},
   "source": [
    "Now we can create random indexes for a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "abd6c744-0bf3-406e-8a05-d1ce07fc0aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "dcd8dbaf-0735-464a-bd69-95c6d7cea4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    \"\"\" Creates batch size Sampler slices. \"\"\"\n",
    "    def __init__(self, sampler, bs, drop_last=False):\n",
    "        fc.store_attr()\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns batches of indexes \"\"\"\n",
    "        yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6735a711-ff25-4111-af92-d48b83658d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29348, 35338, 48222, 29460, 44853, 23000, 686, 9575, 48456, 8354],\n",
       " [29636, 35043, 14874, 4380, 13282, 31218, 25690, 1776, 22751, 8994],\n",
       " [11889, 6814, 3475, 24362, 21516, 34221, 29525, 29915, 45333, 36024],\n",
       " [42387, 12043, 46853, 22491, 4885, 9829, 31501, 33843, 36958, 22575],\n",
       " [42556, 5201, 5470, 19905, 13092, 28590, 9475, 17238, 34363, 23070],\n",
       " [12236, 812, 16667, 18688, 11400, 47723, 4271, 12877, 24966, 28019],\n",
       " [36065, 27811, 18273, 44961, 21658, 23411, 6569, 13712, 19907, 8099],\n",
       " [47989, 16975, 6621, 6945, 47815, 21232, 17006, 7387, 43863, 4304],\n",
       " [47577, 7338, 24473, 15429, 12262, 3224, 4245, 2511, 49857, 12969],\n",
       " [45787, 5109, 12028, 21551, 35988, 46082, 19365, 48860, 25614, 7778]]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 10)\n",
    "list(islice(batchs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe147c88-7334-4891-bedd-88b618f6e545",
   "metadata": {},
   "source": [
    "Now we can create batches of random indexes using `Sampler()` and `BatchSampler()`.\n",
    "\n",
    "However, we need to be returning `torch.Tensors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "903d07d6-de03-4c90-b119-1713d60e2244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    \"\"\" Splits the x and y into the different tensors \"\"\"\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d55a55e0-8fe1-4e49-9bf6-2813bfe04402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate):\n",
    "        fc.store_attr() # Stores everything in __init__() in a self variable\n",
    "        \n",
    "    def __iter__(self):\n",
    "        yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "6c5b59c2-81ad-4fc7-8a6b-771fe59f4ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the batch size indexes\n",
    "train_sample = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_sample = BatchSampler(Sampler(train_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "204da9ec-0297-497e-a949-76cf8b44954f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the actual iterators over the training and validation Datasets\n",
    "train_dl = DataLoader(train_ds, batchs=train_sample)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ce94bd11-57ac-47fd-abbf-cfe3beb21fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl)) # That should fetch an xb shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2612e162-d899-4e4d-b8de-20318234a4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "48d25ef6-b076-43ba-b3a8-e270dfa58ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe56a38c790>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[50].view(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e60de7b2-df23-4304-a18e-0d618e0e4891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "7b038c2d-7d19-481b-80ac-6309245351e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.875 \t loss:0.50107\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.08248\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.12113\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b15653-5604-49ab-aebc-4c35b4b6c3cd",
   "metadata": {},
   "source": [
    "The `PyTorch`'s `DataLoaders` work exactly like the implementation above. However, it runs in parallel when the `DataLoader` class.\n",
    "\n",
    "**I skipped implementing the parallel `DataLoader`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca213d4-e55e-4aac-a825-819704615d6e",
   "metadata": {},
   "source": [
    "### Going to `PyTorch` `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b590b749-98a5-4d42-827b-2902eb1407b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c4fab000-7097-45fb-8d46-c980d2790c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the batch size indexes\n",
    "train_sample = BatchSampler(RandomSampler(train_ds), batch_size=bs, drop_last=False)\n",
    "valid_sample = BatchSampler(Sampler(train_ds, shuffle=False), batch_size=bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "306b7f5a-5708-4abc-b1cf-ac5f56cec2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoaders using the indexes from above\n",
    "train_dl = DataLoader(train_ds, batch_sampler=train_sample, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_sample, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "9c0b9472-15a6-44f2-bcb1-a8e026412511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.938 \t loss:0.28049\n",
      "epoch:1 \t accuracy:1.000 \t loss:0.09918\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.01024\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deec4d-224c-4a5a-8178-23b3d6344f70",
   "metadata": {},
   "source": [
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "418aa2da-302d-430e-a3fb-278279873614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "74d102ea-3dbc-4a77-b337-b623337152ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.000 \t loss:2.38125\n",
      "epoch:1 \t accuracy:0.000 \t loss:2.73254\n",
      "epoch:2 \t accuracy:1.000 \t loss:1.69010\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e458319-5941-4c83-9d76-683f926baa29",
   "metadata": {},
   "source": [
    "#### Shorter version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "66814c23-6b66-4ca7-b41d-5e0d3d968532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A much shorter version which does everything together\n",
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "2e76265f-d01c-44f2-92ee-94bfbd0fcb66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.922 \t loss:0.21675\n",
      "epoch:1 \t accuracy:0.938 \t loss:0.11969\n",
      "epoch:2 \t accuracy:0.984 \t loss:0.04421\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9778cf-2500-4e0c-b931-99be74b27922",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "We've improved out basic training loop. Now, we need to see how it's performing on the validation dataset.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "83aaa459-efa8-40a6-80c2-d919d63c3ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            train_preds = model(xb)\n",
    "            train_loss = loss_func(train_preds, yb)\n",
    "            train_loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        # Print training report\n",
    "        report(epoch, train_preds, yb, train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb, yb in valid_dl:\n",
    "                valid_preds = model(xb)\n",
    "                n = len(xb) # batch size?\n",
    "                count += n # Why?\n",
    "                tot_loss += loss_func(valid_preds, yb).item()*n # Why multiply?\n",
    "                tot_acc += accuracy(valid_preds, yb)*n\n",
    "        print(f\"epoch:{epoch}\\t valid accuracy: {tot_acc/count:.3}\\t valid loss: {tot_loss/count:.3}\")\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "02040f74-2840-484a-9ebf-c74a5aa702e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2, **kwargs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2, **kwargs)\n",
    "    return(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "88cb5820-50ef-47b8-b20f-3a7ec15f0ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "671c03d9-eeeb-4895-8405-3a2d1802251f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.953 \t loss:0.13715\n",
      "epoch:0\t valid accuracy: 0.951\t valid loss: 0.161\n",
      "epoch:1 \t accuracy:0.984 \t loss:0.04575\n",
      "epoch:1\t valid accuracy: 0.965\t valid loss: 0.126\n",
      "epoch:2 \t accuracy:1.000 \t loss:0.01689\n",
      "epoch:2\t valid accuracy: 0.965\t valid loss: 0.116\n",
      "epoch:3 \t accuracy:1.000 \t loss:0.04276\n",
      "epoch:3\t valid accuracy: 0.969\t valid loss: 0.105\n",
      "epoch:4 \t accuracy:0.984 \t loss:0.05489\n",
      "epoch:4\t valid accuracy: 0.97\t valid loss: 0.103\n",
      "CPU times: user 16.4 s, sys: 1.02 s, total: 17.4 s\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%time loss, acc = fit(5, model, F.cross_entropy, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327f734-670c-4262-8b99-10311781355b",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1278679a-8dda-4089-b1d8-fab03b5a6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb55c3-b009-40fa-8257-82ddb973a037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
