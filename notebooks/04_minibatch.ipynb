{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707bcdbe-22f0-49b3-8dc7-c659c2ea3431",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Minibatch\n",
    "\n",
    "\n",
    "Why are we re-writing things from scratch?\n",
    "\n",
    "The idea is that we're trying to learn a library better. If we can't learn what the library does,  we can just re-implement the algorithm ourselves and then we'll be able to understand the library's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fbab1c-8e99-4d54-944a-48ec19323069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbdbc34-ab5e-46a6-95f9-0a12cae1e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Get the MNIST dataset\n",
    "import gzip, pickle\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MNIST_URL = \"https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true\"\n",
    "data_path = Path(\"../data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "data_gz = data_path/\"mnist.pkl.gz\"\n",
    "\n",
    "# Get the data\n",
    "if not data_gz:\n",
    "    urllib.request.urlretrieve(MNIST_URL, data_path/\"mnist.pkl.gz\")\n",
    "    \n",
    "# Destructuring\n",
    "with gzip.open(data_gz, mode='rb') as unzip_data:\n",
    "    obj = pickle.load(unzip_data, encoding=\"latin-1\")\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = obj\n",
    "\n",
    "# To tensors\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e20edea-32a5-4222-b178-084a0ffe33da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = int(y_train.max()+1)\n",
    "nh = 50\n",
    "n, m, c, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d3b312-bf06-4f05-85b9-b4bdd936c37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our model architecture from 03_backpropagation.ipynb\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [\n",
    "            nn.Linear(n_in, nh), # [784,50]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nh, n_out) # [50,10]\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27adc51d-9e93-4b95-9363-34c03f952832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, c)\n",
    "preds = model(x_train)\n",
    "preds.shape # We are now going to use 10 categories for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c34768-f492-43cb-a0af-79f0046f223c",
   "metadata": {},
   "source": [
    "## Implement `Cross Entropy Loss` function\n",
    "\n",
    "In the last notebook, we used the MSE (L2-norm) to calculate the error. That function was not ideal for predicting multicategory classifications, like MNIST. Following, we are going to reconstruct the `cross entropy loss` function from scratch. \n",
    "\n",
    "The targets will be `on-hot-encoded` vectors with the index of a 1 representing the actual number.\n",
    "\n",
    "**`Cross Entropy Loss` typically serves multi-class and multi-label classifications.**\n",
    "\n",
    "`Cross Entropy Loss = log(SoftMax(i))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708c08b-4294-4008-9568-fd6fd6a9b39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we will need to compute the softmax of our activations. This is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$ \n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30ef424-4573-44a5-9da9-7817257cc995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# softmax: x.exp()/(x.exp().sum(dim=-1, keepdim=True))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Softmax \"\"\"\n",
    "    return (x.exp()/(x.exp().sum(dim=-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07ebf3-40ac-4543-be54-abcccd956b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62f1635-6451-436d-ab2b-ef801fc87afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#log(softmax(x)):\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dac5435-ced7-4bf6-a6b3-465a67a01538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2946, -2.2858, -2.3051,  ..., -2.2757, -2.1882, -2.3780],\n",
       "        [-2.3487, -2.1997, -2.2733,  ..., -2.2251, -2.3024, -2.4504],\n",
       "        [-2.3303, -2.3740, -2.2654,  ..., -2.2065, -2.2417, -2.3584],\n",
       "        ...,\n",
       "        [-2.3527, -2.2916, -2.2930,  ..., -2.2117, -2.2450, -2.4061],\n",
       "        [-2.2955, -2.2924, -2.2037,  ..., -2.2864, -2.1833, -2.3594],\n",
       "        [-2.3670, -2.3110, -2.3027,  ..., -2.2629, -2.2387, -2.3645]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds = log_softmax(preds)\n",
    "log_softmax_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83731144-ecc3-47ee-8a0b-7f6afede400b",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "In `PyTorch` this is known as : negative log likelihood loss == `nll`\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807cfd21-17a2-4e8b-9b74-0187ae04b938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 3 numbers\n",
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2013c7-e899-4279-9103-fce7ab8c98d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c2b741-ab3d-470f-85f7-19a8d2e8ae10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.3164, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.3487, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.1563, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prababilities of the categories from above are\n",
    "log_softmax_preds[0, 5], log_softmax_preds[1, 0], log_softmax_preds[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f900fddc-41ab-4ac7-bc07-766f95bb363d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.3164, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.3487, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.1563, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[0, y_train[0]], log_softmax_preds[1, y_train[1]], log_softmax_preds[2, y_train[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4751548-7589-41bb-b5a4-7a2df9d7e68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3164, -2.3487, -2.1563], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax_preds[[0,1,2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c479ba9-c851-43ab-accb-60c0fc78598a",
   "metadata": {},
   "source": [
    "So, we have the `log(softmax(p))`. Now we need to implement the Cross entropy loss from above as the sum of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f94afa-f14e-4203-a7b4-bce7c7ee01d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nll(inp, target):\n",
    "    \"\"\" Cross Entropy Loss \"\"\"\n",
    "    return -inp[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49fe07f-65be-49d9-a62f-86501697f504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3083, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(log_softmax_preds, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57448954-1669-4a23-b31b-30c63c68ad16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Going to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c906e-4b44-4bd4-a726-89ea6e75746a",
   "metadata": {},
   "source": [
    "`PyTorch` already has a function for `Cross Entropy Loss`, so we can use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef8efe8-3830-4b0e-b7d1-d5e9df840c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3083, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# nll = negative log likelihood loss\n",
    "F.nll_loss(F.log_softmax(preds, dim=1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e861-e21a-4ea0-9557-bd0adf4de8b3",
   "metadata": {},
   "source": [
    "The above combination of the 2 functions can be combined into 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487d960c-da9e-4ad2-9a25-50ef2e878cca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3083, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f6b92-af96-4603-921e-741c3a5b01c0",
   "metadata": {},
   "source": [
    "## Basic training loop\n",
    "\n",
    "1) get the predictions from the model\n",
    "2) calculate the loss from the predictions (based on y_train)\n",
    "3) calculate the gradients of the loss with respect to every parameter of the model\n",
    "4) adjust the parameters according to their gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca9d642-5bc9-40c6-9aa3-23e136b27f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa881109-ee28-487c-9f56-0cefacbc5c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0261,  0.0350,  0.0157, -0.1338,  0.1903,  0.0043, -0.1223,  0.0450,\n",
       "          0.1325, -0.0572], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "xb = x_train[:bs] # minibatch from x\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f046e1c4-ac61-47f1-8917-6bf0a52492f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[:bs]\n",
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a79f255b-a44c-4926-b807-103aed69a9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3180, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3635a4-7bb0-4149-a001-193b89a1b406",
   "metadata": {
    "tags": []
   },
   "source": [
    "We need to grab the highest probabilities of the predicted nubmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f921d26-059d-4d99-b3b0-2f8e2e861e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 7, 2, 4, 8, 4, 4, 4, 4, 7, 4, 8, 4, 4, 4, 4, 2, 4, 2, 1, 4, 7,\n",
       "        8, 7, 4, 4, 4, 4, 8, 4, 4, 4, 4, 4, 8, 4, 4, 8, 4, 4, 8, 4, 8, 4, 8, 4,\n",
       "        4, 4, 2, 4, 7, 4, 4, 4, 4, 4, 2, 7, 4, 2, 2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the indexes of the highest probabilities\n",
    "# preds.argmax(dim=1)\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362455d-3fa0-419b-9e97-3966800aa603",
   "metadata": {},
   "source": [
    "And we define the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e4506e-24d5-4f31-b26b-a5c5b70e97b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def accuracy(preds:torch.tensor, targs:torch.tensor) -> float:\n",
    "    \"\"\" The average accuracy of the correctly predicted numbers \"\"\"\n",
    "    return (preds.argmax(dim=1)==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8268c9f1-b240-4121-b0a1-dde89b17affb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1094)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c244910-25dc-4e1d-8fed-6c4c10fff62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def report(epoch:int, preds:torch.tensor, targs:torch.tensor, loss:float):\n",
    "    \"\"\" Print a report \"\"\"\n",
    "    print(f\"epoch:{epoch}\\n#-------------------------#\")\n",
    "    print(f\"accuracy:{accuracy(preds, targs).item():.3f} \\t loss:{loss.item():.5f}\")\n",
    "    print(\"#=========================#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bf2c580-ba44-4879-86b1-5c425a2017a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.109 \t loss:2.31799\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "xb, yb = x_train[:bs], y_train[:64]\n",
    "preds = model(xb)\n",
    "report(1, preds, yb, loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4d8c3-1587-479d-a2fd-7889d6140d86",
   "metadata": {},
   "source": [
    "Let's setup the basic training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2203c8da-124f-4efd-9f55-378740cdadcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recreate the model before running the training\n",
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18d4cb29-f186-4617-99f4-14a2ae088046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 \t accuracy:0.937500 \t loss:0.350084\n",
      "epoch:1 \t accuracy:0.875000 \t loss:0.191585\n",
      "epoch:2 \t accuracy:1.000000 \t loss:0.054027\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        \n",
    "        # Create xb, yb batch - this operation is way slower than slice\n",
    "        if i+batch_size > x_train.shape[0]:\n",
    "            xb = x_train[i:]\n",
    "            yb = y_train[i:]\n",
    "        else:\n",
    "            xb = x_train[i:i+batch_size] # x batch\n",
    "            yb = y_train[i:i+batch_size] # y batch\n",
    "            \n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    print(f\"epoch:{epoch} \\t accuracy:{accuracy(preds, yb).item():.6f} \\t loss:{loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830b391-71ed-4a58-b650-865aebebc756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using `slice`\n",
    "\n",
    "The `slice()` function returns a `slice object` that is used to slice any sequence (string, tuple, list, range, or bytes). So, we can use it to slice the training tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2712c226-61c2-4962-9245-375d797f5efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9186d6de-f896-4283-80b5-eac793197016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.27012\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.19202\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.12710\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "n_inp = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n_inp, 64):\n",
    "        s = slice(i, min(n_inp, i + bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4062cd-8fa0-45e1-a9bf-73869919bbb7",
   "metadata": {},
   "source": [
    "### PyTorch parameters\n",
    "\n",
    "We are going to reduce the code size for the parameter update.\n",
    "\n",
    "If we create an object which inherits from `nn.Module`, we can assign arbitrary parameters to it and traverse them as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eba11b9c-f657-4d38-b119-15c961aec37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(\n",
       "  (foo): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (bar): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = nn.Module()\n",
    "m1.foo = nn.Linear(3, 4)\n",
    "m1.bar = nn.Linear(4, 1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9a08786-500e-489a-a17f-8e8e60330204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foo', Linear(in_features=3, out_features=4, bias=True)),\n",
       " ('bar', Linear(in_features=4, out_features=1, bias=True))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows us to access the attributes of m1\n",
    "list(m1.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0299eae-8091-41a6-b784-e3c6b3dff9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.2024, -0.1002, -0.0442],\n",
       "         [ 0.2234,  0.3811,  0.1495],\n",
       "         [ 0.3320,  0.1581,  0.1009],\n",
       "         [-0.0522, -0.5572, -0.4191]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1143, -0.1830, -0.3572, -0.5359], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.4269, -0.4508,  0.1501,  0.2800]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4575], requires_grad=True)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prints the weights and bias matrices\n",
    "list(m1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4502bac4-ad18-4e8f-a7d1-52d5d9bbf7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi Layer Perceptron to illustrate the use of parameter. \n",
    "    A MLP has a minimum of 3 layers which makes this one the simplest MLP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        return self.l2(self.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cf5f17d-bf18-4c7e-a92b-881e7d5cd204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(m, nh, c)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f55128d-2df0-459b-bb87-b639859d58c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n",
      "relu: ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, l in model.named_children():\n",
    "    print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf07a0f-e000-41f6-966c-a576e8b0864f",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can access the parameteres (`weights` and `biases`) of each layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf5bb8f-90f7-4e1c-bdbc-bc1fcaf694bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e18ef-c1cc-485e-9f31-620ea430db57",
   "metadata": {},
   "source": [
    "That means, we can update them in the loop we hade defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f7ba91b-8c7e-41ba-98e5-cf0c42f353f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MLP(m, nh, c)\n",
    "bs = 64\n",
    "epochs = 3\n",
    "lr = 0.5\n",
    "n_inp = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bc366d7-a2d4-449e-a2c7-fe53cc530519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model=model, epochs=3):\n",
    "    \"\"\" Basic loop using Torch model parameters to update their gradients. \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n_inp, bs):\n",
    "            s = slice(i, min(n_inp, i+bs))\n",
    "            xb = x_train[s]\n",
    "            yb = y_train[s]\n",
    "            preds = model(xb) # We are now using the MLP model\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad*lr\n",
    "                model.zero_grad() # Reset gradients for all model parameters\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9211cdef-9694-4e34-83e0-6fef47ea52b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.22061\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.10989\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.05784\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49880cec-c432-4962-a632-68658265a0da",
   "metadata": {},
   "source": [
    "#### How it works in the background\n",
    "\n",
    "In the background `PyTorch` uses the `__setattr__()` function which allows you to set arbitrary object properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e610d333-6216-4e03-9473-2f0643b8b69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self, k, v):\n",
    "        \"\"\" This is how PyTorch.nn.Module stores the children of the model. \"\"\"\n",
    "        if not k.startswith(\"_\"):\n",
    "            self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self._modules}\"\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\" How to traverse parametes \"\"\"\n",
    "        for l in self._modules.values():\n",
    "            yield from l.parameters()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bcf22c1-f957-48e3-926d-a1744c0b02eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel = MyModule(m, nh, c)\n",
    "mymodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bb4b920-2837-4526-b79e-4f977752c985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in mymodel.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532e8c4-82b9-4684-8067-3bf5ea8dddb1",
   "metadata": {},
   "source": [
    "### Registering layers in model - from scratch\n",
    "\n",
    "How do we registers all the layers in our model at once - with `nn.Module`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9637faa8-6bdb-49d2-97f8-f5806d40aa1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppose we store the layers in a variable\n",
    "layers = [nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46289d90-2417-4919-9684-f93c682baa24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, l in enumerate(self.layers):\n",
    "            self.add_module(f\"layer_{i}\", l) # This adds the modules to the module\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d36a685f-a516-4f75-af42-864ea15a8fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6906215-211a-4b1b-b932-c283771da10b",
   "metadata": {},
   "source": [
    "It's apparent that **we're building a sequential model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cbf73-6978-4fe5-b129-d31cf5f91847",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `nn.ModuleList()`\n",
    "\n",
    "`PyTorch` allows use to add module with the `nn.ModuleList()` function.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#modulelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75e9e879-2286-4b5d-8da3-348c55b14d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList(layers) # Create sequential model\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = l(x)\n",
    "        return x\n",
    "        # return reduce(lambda val,layer: layer(val), self.layers, x)\n",
    "    # Python reduce: https://realpython.com/python-reduce-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc15addb-92de-4a72-9943-aff433a2929a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqmodel = SequentialModel(layers)\n",
    "seqmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30f5c9fe-650d-4eb8-8b02-4072c28e083c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.812 \t loss:0.36627\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.15036\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.13897\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit(model=seqmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bc782-3db1-471b-a71d-f1ad70375d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `nn.Sequential()`\n",
    "\n",
    "This is a convenience class which does the same thing as above - it registers our modules for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "097275c0-be0f-44d0-af77-245e1329d2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e17ff4b-60e4-4c8d-b078-57e27ea6f827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.18467\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.10290\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.05294\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfe185-ceaa-439f-8057-8bea42d3e878",
   "metadata": {},
   "source": [
    "### SGD Optimizer - from scratch\n",
    "\n",
    "Finding the gradients of the layers and updating the parameters is a common step. An optimizer is a function which does that for us.\n",
    "\n",
    "```Python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= p.grad*lr\n",
    "    model.zero_grad() # Reset gradients for all model parameters\n",
    "```\n",
    "\n",
    "Let's create our own `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d930256a-2fcc-4ff8-9edd-09baf19c2691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD_Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params:\n",
    "                p -= p.grad * self.lr\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ed54b23-9754-4437-9f78-6aa4a36005d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a016ea31-c812-44f2-b441-9822d2c57366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create opt as an object of the model's parameters\n",
    "opt = SGD_Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1220c91a-aa78-4d14-8812-f7ed8a9afab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.21141\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.875 \t loss:0.13728\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.04574\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "# New training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(i+bs, n))\n",
    "        xb = x_train[s]\n",
    "        yb = y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Out custom SGD optimizer\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5795599-cea4-4fe0-9d8b-f96c5ddef263",
   "metadata": {
    "tags": []
   },
   "source": [
    "PyTorch provides the same functionality with `optim.SGD` and it also provides the `momentum` parameter (which avoids local minimums during SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf08ae2-f3b0-4273-85e6-d803bc6feca0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Going to PyTorch - `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e0d1133-b626-4b8e-b90a-6edb3089be1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9adc040-1c7d-410b-b25c-1919c52be9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "opt = optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b3ef14c-f08f-4c73-9975-a1ab0b284a98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.875 \t loss:0.21303\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.11312\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.05455\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(i+bs, n))\n",
    "        xb = x_train[s]\n",
    "        yb = y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = F.cross_entropy(preds, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # PyTorch's SGD optimizer\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dbfb062-ba1f-438d-b183-1d978646564e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_lin_model():\n",
    "    \"\"\" Use PyTorch to create the smallest MLP and SGD optimizer. \"\"\"\n",
    "    _model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "    _opt = optim.SGD(_model.parameters(), lr=0.5)\n",
    "    return _model, _opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3842aa-dbce-4951-a25a-08c4d33e4be0",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "We're going to create the `Dataset` and `DataLoader` from scratch to understand how they're implemented in `PyTorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bb5bc-b130-4ffb-9dca-143fd81d5d00",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We'll go from:\n",
    "\n",
    "```python\n",
    "s = slice(i, min(i+bs, n))\n",
    "xb = x_train[s]\n",
    "yb = y_train[s]\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "xb,yb = train_ds[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "138e3750-e64b-4452-aac7-75cc029a15b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, s):\n",
    "        \"\"\" Slicing function: replaced slice() \"\"\"\n",
    "        return self.x[s], self.y[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a186c205-ed87-4e01-9f02-13f3bc0c55f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds) == len(x_train)\n",
    "assert len(valid_ds) == len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e22da11-38f2-496f-ad66-024288c06871",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape == (5, 28*28)\n",
    "assert yb.shape == (5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7027e1b4-6f42-44ec-b199-5f495f07b083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de2ccc36-758d-4768-91a6-0f95dc8d8294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New fit function with Dataset class\n",
    "def fit(model = model, opt = opt):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, n, bs):\n",
    "            xb, yb = train_ds[i:min(n, i+bs)]\n",
    "            preds = model(xb)\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad01d82f-878d-421d-b2c5-85169423255a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.20560\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.10961\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.05015\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105db3c3-1f15-4e8d-90ad-cdb45b0ea2f1",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "We are going to replace:\n",
    "\n",
    "```Python\n",
    "for i in range(0, n, bs):\n",
    "    xb, yb = train_ds[i:min(n, i+bs)]\n",
    "```\n",
    "with:\n",
    "```Python\n",
    "for xb, yb in train_dl:\n",
    "    ...\n",
    "```\n",
    "\n",
    "So we need to create an itterator with `__iter__()` and `yield`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cbd67d1-3773-4353-b947-e64905bd287e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs):\n",
    "            yield self.ds[i:min(len(self.ds), i+self.bs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33330811-9123-47e3-8ec5-76ce4c2ee88d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "978daa37-7670-47c7-ab75-cb5966618092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4512aad-f598-40fa-b39c-7d30e96a83a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05de6684-0e80-4911-9d05-5125c7a46289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f23b9720e20>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[7].view(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e62c773a-5205-4a8f-823f-b32770b50613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48c244c7-dbd0-4d72-b9a7-c4ec7d4bdea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = F.cross_entropy(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(epoch, preds, yb, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cdb2b70-0b8e-48f7-9c2e-a7f2c6f51a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.26585\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.10045\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.04141\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b567e5-eae4-4681-9176-634167203a36",
   "metadata": {},
   "source": [
    "### Batch randomizer\n",
    "\n",
    "Now, we have created a way to load datasets into the traininig function. However, the DataLoaderes are sequential - they fetch items in a sequence.\n",
    "\n",
    "One feature we can add to the `DataLoader` is a shuffler, which fetches random data from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1728c9e5-e45f-4864-8d22-9ee4ab3019f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a69a98be-89b5-4f83-8a32-f76dd23961bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    \"\"\" Returns an iterator with optionally randomized indexes of the Dataset \"\"\"\n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the indexes which we're going to use to create xb and yb \"\"\"\n",
    "        result = list(range(self.n))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(result)\n",
    "        return iter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54852296-e71c-4510-9e6f-60e3b6e79047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04b76b86-f2e9-4b7e-b6e7-b5fa91dfd9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds) # If shuffle==False\n",
    "list(islice(ss, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "103daefa-4a07-4157-98cf-a71887a6393d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36765, 21559, 45671, 48889, 15611]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Sampler(train_ds, shuffle=True)\n",
    "list(islice(ss,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a448c-ecce-4521-80ad-d9ed2e4c0dc0",
   "metadata": {},
   "source": [
    "Now we can create random indexes for a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "abd6c744-0bf3-406e-8a05-d1ce07fc0aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcd8dbaf-0735-464a-bd69-95c6d7cea4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchSampler():\n",
    "    \"\"\" Creates batch size Sampler slices. \"\"\"\n",
    "    def __init__(self, sampler, bs, drop_last=False):\n",
    "        fc.store_attr()\n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns batches of indexes of batch size \"\"\"\n",
    "        yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6735a711-ff25-4111-af92-d48b83658d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11864, 44562, 47173, 34958, 35813],\n",
       " [23133, 14792, 26436, 5074, 6578],\n",
       " [18456, 49084, 16361, 6593, 48658],\n",
       " [34579, 19805, 44082, 9564, 45089],\n",
       " [24511, 23982, 34565, 23215, 12153],\n",
       " [25217, 35420, 19097, 29545, 33177],\n",
       " [19103, 47457, 29341, 37032, 33453],\n",
       " [23367, 5834, 17416, 46802, 41554],\n",
       " [3610, 17476, 18338, 5032, 24122],\n",
       " [28267, 25847, 19988, 12025, 35943]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchs = BatchSampler(ss, 5)\n",
    "list(islice(batchs, 10))\n",
    "# Now we can generate x amount of randomized indexes of batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe147c88-7334-4891-bedd-88b618f6e545",
   "metadata": {},
   "source": [
    "Now we can create batches of random indexes using `Sampler()` and `BatchSampler()`.\n",
    "\n",
    "However, we need to be returning `torch.Tensors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "903d07d6-de03-4c90-b119-1713d60e2244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    \"\"\" Splits the x and y into the different tensors \"\"\"\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2a28620-0c80-426a-97a3-c1eb723f95c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  3.,  6., 10.],\n",
       "        [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.,3.,6.,10.])\n",
    "b = torch.tensor([0, 0, 0, 0, ])\n",
    "torch.stack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d55a55e0-8fe1-4e49-9bf6-2813bfe04402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, batchs, collate_fn=collate):\n",
    "        fc.store_attr() # Stores everything in __init__() in a self variable\n",
    "        \n",
    "    def __iter__(self):\n",
    "        yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc84ce07-5e5c-4eb8-8b9b-c71409f08e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c5b59c2-81ad-4fc7-8a6b-771fe59f4ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the batch size indexes\n",
    "train_sample = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_sample = BatchSampler(Sampler(train_ds, shuffle=False), bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bfdb0225-5d8b-4d87-97e4-554a6b34fc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[36713,\n",
       "  22015,\n",
       "  35353,\n",
       "  39830,\n",
       "  10120,\n",
       "  37044,\n",
       "  21093,\n",
       "  15158,\n",
       "  12016,\n",
       "  45263,\n",
       "  27426,\n",
       "  26519,\n",
       "  5554,\n",
       "  49232,\n",
       "  31819,\n",
       "  25300,\n",
       "  34586,\n",
       "  990,\n",
       "  21485,\n",
       "  5201,\n",
       "  20972,\n",
       "  43110,\n",
       "  36652,\n",
       "  35119,\n",
       "  39661,\n",
       "  22562,\n",
       "  29229,\n",
       "  47198,\n",
       "  16237,\n",
       "  2368,\n",
       "  29410,\n",
       "  33753,\n",
       "  16455,\n",
       "  25428,\n",
       "  11622,\n",
       "  7558,\n",
       "  2559,\n",
       "  43946,\n",
       "  16775,\n",
       "  17098,\n",
       "  32618,\n",
       "  10392,\n",
       "  34852,\n",
       "  31394,\n",
       "  7338,\n",
       "  18980,\n",
       "  45656,\n",
       "  13525,\n",
       "  10848,\n",
       "  45110,\n",
       "  32637,\n",
       "  33249,\n",
       "  4157,\n",
       "  21181,\n",
       "  32907,\n",
       "  43573,\n",
       "  45678,\n",
       "  16476,\n",
       "  42483,\n",
       "  45210,\n",
       "  35678,\n",
       "  49847,\n",
       "  15567,\n",
       "  24512]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(train_sample, 1)) # 1x64 list of randomized indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "204da9ec-0297-497e-a949-76cf8b44954f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the actual iterators over the training and validation Datasets\n",
    "train_dl = DataLoader(train_ds, batchs=train_sample)\n",
    "valid_dl = DataLoader(valid_ds, batchs=valid_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ce94bd11-57ac-47fd-abbf-cfe3beb21fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl)) # That should fetch an xb shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2612e162-d899-4e4d-b8de-20318234a4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape # These are randomized training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48d25ef6-b076-43ba-b3a8-e270dfa58ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f23b93d25c0>, tensor(9))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[50].view(28, 28), cmap=\"gray\"), yb[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e60de7b2-df23-4304-a18e-0d618e0e4891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7b038c2d-7d19-481b-80ac-6309245351e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.938 \t loss:0.16972\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.06704\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.00487\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b15653-5604-49ab-aebc-4c35b4b6c3cd",
   "metadata": {},
   "source": [
    "The `PyTorch`'s `DataLoaders` work exactly like the implementation above. However, it runs in parallel when the `DataLoader` class.\n",
    "\n",
    "**I skipped implementing the parallel `DataLoader`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca213d4-e55e-4aac-a825-819704615d6e",
   "metadata": {},
   "source": [
    "### Going to `PyTorch` `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b590b749-98a5-4d42-827b-2902eb1407b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4fab000-7097-45fb-8d46-c980d2790c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the batch size indexes\n",
    "train_sample = BatchSampler(RandomSampler(train_ds),     bs, drop_last=False)\n",
    "valid_sample = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "306b7f5a-5708-4abc-b1cf-ac5f56cec2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the DataLoaders using the indexes from above\n",
    "train_dl = DataLoader(train_ds, batch_sampler=train_sample, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_sample, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c0b9472-15a6-44f2-bcb1-a8e026412511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.02923\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.00983\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:0.875 \t loss:0.24566\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deec4d-224c-4a5a-8178-23b3d6344f70",
   "metadata": {},
   "source": [
    "OR\n",
    "\n",
    "PyTorch's `DataLoader` can create the (random) batch samples for us - so we can use it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "418aa2da-302d-430e-a3fb-278279873614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "74d102ea-3dbc-4a77-b337-b623337152ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.05337\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.08325\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:1.000 \t loss:0.00162\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbed9c4-b7ce-4bb8-9dd2-a817c0d2f8db",
   "metadata": {},
   "source": [
    "`PyTorch` `DataLoader` can perform the random batch sampling without the having to explicitly create the `RandomSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f751dfbf-7c47-441b-8b6d-54490a93ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d89cb34-506f-493a-b98f-4ed3844f0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a4bf072-817d-4bbe-8f76-18185c1e3adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.984 \t loss:0.07013\n",
      "#=========================#\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.969 \t loss:0.10339\n",
      "#=========================#\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:0.969 \t loss:0.13232\n",
      "#=========================#\n"
     ]
    }
   ],
   "source": [
    "model, opt, get_lin_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9778cf-2500-4e0c-b931-99be74b27922",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "We've improved out basic training loop. Now, we need to see how it's performing on the validation dataset.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c46399a6-85e4-4cb5-9b5d-514746caa0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.Tensor([1,2,3,4])\n",
    "T2 = torch.Tensor([5,6, 7,8])\n",
    "T3 = torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60e1ff6f-886d-4d38-9d9f-98bca61d56e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T3 = torch.cat((T3, T1), dim=0)\n",
    "T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "83aaa459-efa8-40a6-80c2-d919d63c3ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_total_loss = 0\n",
    "        nof_samples = 0\n",
    "        cumulative_preds = torch.Tensor()\n",
    "        cumulative_targs = torch.Tensor()\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "            train_preds = model(xb)\n",
    "            cumulative_preds = torch.cat((cumulative_preds, train_preds))\n",
    "            cumulative_targs = torch.cat((cumulative_targs, yb))\n",
    "            train_loss = loss_func(train_preds, yb)\n",
    "            nof_samples += 1\n",
    "            epoch_total_loss += train_loss\n",
    "            train_loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(\"Training report\")\n",
    "        report(epoch, cumulative_preds, cumulative_targs, epoch_total_loss/nof_samples)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc, count = 0.,0.,0\n",
    "            for xb, yb in valid_dl:\n",
    "                valid_preds = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(valid_preds, yb).item()*n # Why multiply?\n",
    "                tot_acc += accuracy(valid_preds, yb)*n\n",
    "        print(\"Validation report\")\n",
    "        print(f\"valid accuracy: {tot_acc/count:.3}\\t valid loss: {tot_loss/count:.3}\\n\")\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "02040f74-2840-484a-9ebf-c74a5aa702e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True, num_workers=2, **kwargs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=2, **kwargs)\n",
    "    return(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "88cb5820-50ef-47b8-b20f-3a7ec15f0ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, opt = get_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "671c03d9-eeeb-4895-8405-3a2d1802251f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training report\n",
      "epoch:0\n",
      "#-------------------------#\n",
      "accuracy:0.904 \t loss:0.30996\n",
      "#=========================#\n",
      "Validation report\n",
      "valid accuracy: 0.923\t valid loss: 0.24\n",
      "\n",
      "Training report\n",
      "epoch:1\n",
      "#-------------------------#\n",
      "accuracy:0.957 \t loss:0.14151\n",
      "#=========================#\n",
      "Validation report\n",
      "valid accuracy: 0.966\t valid loss: 0.116\n",
      "\n",
      "Training report\n",
      "epoch:2\n",
      "#-------------------------#\n",
      "accuracy:0.967 \t loss:0.10679\n",
      "#=========================#\n",
      "Validation report\n",
      "valid accuracy: 0.967\t valid loss: 0.109\n",
      "\n",
      "Training report\n",
      "epoch:3\n",
      "#-------------------------#\n",
      "accuracy:0.972 \t loss:0.08757\n",
      "#=========================#\n",
      "Validation report\n",
      "valid accuracy: 0.969\t valid loss: 0.104\n",
      "\n",
      "Training report\n",
      "epoch:4\n",
      "#-------------------------#\n",
      "accuracy:0.976 \t loss:0.07607\n",
      "#=========================#\n",
      "Validation report\n",
      "valid accuracy: 0.968\t valid loss: 0.109\n",
      "\n",
      "CPU times: user 1min, sys: 16 s, total: 1min 16s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%time loss, acc = fit(5, model, F.cross_entropy, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327f734-670c-4262-8b99-10311781355b",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1278679a-8dda-4089-b1d8-fab03b5a6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb55c3-b009-40fa-8257-82ddb973a037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
